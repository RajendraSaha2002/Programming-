# -*- coding: utf-8 -*-
"""Rajendra_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NWRqQ2Sx2_e8v_1mP77F_4FFi3IzMR7M
"""

!pip install transformers

!pip install torch

!pip install HuggingFace transformers libraries

# Step 1: Import required libraries
from transformers import AutoModelForCausalLM, AutoTokenizer

# Step 3: Load the pretrained DialoGPT model and tokenizer
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Step 4: Define chatbot function
def chatbot():
  print("Chatbot is ready! Type 'exit' to end the conversation.")
  chat_history_ids = None
  while True:
    user_input = input("User: ")
    if user_input.lower() == "exit":
      print("Chatbot: Goodbye!")
      break
    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors="pt")
    if chat_history_ids is not None:
      bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1)

      chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
      response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
      print(f"Chatbot:, {response}")

# Step 5: Run the chatbot
chatbot()

